{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance with PR curves, confusion matrixes, and F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import load\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "sys.path.append(\"../../utils\")\n",
    "from eval_utils import (\n",
    "    generate_confusion_matrix_df,\n",
    "    generate_f1_score_df,\n",
    "    generate_accuracy_score_df,\n",
    ")\n",
    "from training_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths to different datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with datasets\n",
    "data_dir = pathlib.Path(\"../0.train_logistic_regression/data\")\n",
    "\n",
    "# Directory with models\n",
    "models_dir = pathlib.Path(\"../0.train_logistic_regression/models\")\n",
    "\n",
    "# Path to encoder file\n",
    "encoder_path = pathlib.Path(\"../0.train_logistic_regression/encoder_results/label_encoder_log_reg_fs_plate_4.joblib\")\n",
    "\n",
    "# Directory for model figures output\n",
    "fig_dir = pathlib.Path(\"./figures\")\n",
    "fig_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Directory for result outputs (accuracy scores)\n",
    "results_dir = pathlib.Path(\"./results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load in each model individually\n",
    "final_model = load(\n",
    "    pathlib.Path(f\"{models_dir}/log_reg_fs_plate_4_final_downsample.joblib\")\n",
    ")\n",
    "shuffled_model = load(\n",
    "    pathlib.Path(f\"{models_dir}/log_reg_fs_plate_4_shuffled_downsample.joblib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output precision and recall data per dataset applied to each model and combine into one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating shuffled model...\n",
      "Applying model to testing_data ...\n",
      "Applying model to holdout2_data ...\n",
      "Applying model to holdout1_data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/cellpainting_predicts_cardiac_fibrosis/5.machine_learning/1.evaluate_models/../../utils/training_utils.py:60: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_to_data, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying model to training_data ...\n",
      "Evaluating final model...\n",
      "Applying model to testing_data ...\n",
      "Applying model to holdout2_data ...\n",
      "Applying model to holdout1_data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/cellpainting_predicts_cardiac_fibrosis/5.machine_learning/1.evaluate_models/../../utils/training_utils.py:60: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_to_data, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying model to training_data ...\n",
      "(33720, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Data_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374428</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048479</td>\n",
       "      <td>shuffled</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.374529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.068064</td>\n",
       "      <td>shuffled</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.374630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096699</td>\n",
       "      <td>shuffled</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.374731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097081</td>\n",
       "      <td>shuffled</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.374832</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106663</td>\n",
       "      <td>shuffled</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall  Threshold Model_Type Data_Type\n",
       "0   0.374428     1.0   0.048479   shuffled   testing\n",
       "1   0.374529     1.0   0.068064   shuffled   testing\n",
       "2   0.374630     1.0   0.096699   shuffled   testing\n",
       "3   0.374731     1.0   0.097081   shuffled   testing\n",
       "4   0.374832     1.0   0.106663   shuffled   testing"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the subdirectory for this plot type within fig_dir\n",
    "pr_curves_dir = fig_dir / \"pr_curves\"\n",
    "pr_curves_dir.mkdir(parents=True, exist_ok=True)  # Create subdirectory if it doesn't exist\n",
    "\n",
    "# Initialize empty lists to store data for each iteration\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "threshold_list = []\n",
    "model_type_list = []\n",
    "data_type_list = []\n",
    "\n",
    "for model_path in models_dir.iterdir():\n",
    "    if model_path.is_dir() or model_path.suffix != \".joblib\":\n",
    "        continue  # Skip directories or files that are not model files\n",
    "\n",
    "    print(\"Evaluating\", model_path.stem.split(\"_\")[5], \"model...\")\n",
    "    \n",
    "    for data_path in data_dir.iterdir():\n",
    "        print(\"Applying model to\", data_path.stem, \"...\")\n",
    "        # load in model to apply to datasets\n",
    "        model = load(model_path)\n",
    "\n",
    "        # load in label encoder\n",
    "        le = load(encoder_path)\n",
    "\n",
    "        # Load in X and y data from dataset\n",
    "        X, y = load_data(path_to_data=data_path, label=\"Metadata_cell_type\")\n",
    "\n",
    "        # Assign y classes to correct binary using label encoder results\n",
    "        y_binary = le.transform(y)\n",
    "\n",
    "        # predict class probabilities for morphology feature data\n",
    "        predicted_probs = model.predict_proba(X)\n",
    "\n",
    "        # Calculate the precision, recall data\n",
    "        precision, recall, threshold = precision_recall_curve(\n",
    "            y_binary, predicted_probs[:, -1]\n",
    "        )\n",
    "        threshold = np.append(threshold, np.nan)\n",
    "\n",
    "        # Append data to lists\n",
    "        precision_list.extend(precision.tolist())\n",
    "        recall_list.extend(recall.tolist())\n",
    "        threshold_list.extend(threshold.tolist())\n",
    "        model_type_list.extend([model_path.stem.split(\"_\")[5]] * len(precision))\n",
    "        data_type_list.extend([data_path.stem.split(\"_\")[0]] * len(precision))\n",
    "\n",
    "# Create a DataFrame from the accumulated data\n",
    "pr_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Precision\": precision_list,\n",
    "        \"Recall\": recall_list,\n",
    "        \"Threshold\": threshold_list,\n",
    "        \"Model_Type\": model_type_list,\n",
    "        \"Data_Type\": data_type_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Drop any NA data\n",
    "pr_df.dropna(inplace=True)\n",
    "\n",
    "# Show output of all data\n",
    "print(pr_df.shape)\n",
    "pr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precision-recall curves for training and testing data per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curves with only testing and training data\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Combine model and data type as one column for plotting\n",
    "pr_df[\"data_split\"] = pr_df[\"Model_Type\"] + \" (\" + pr_df[\"Data_Type\"] + \")\"\n",
    "\n",
    "# Filter data frame to only show training versus testing\n",
    "filtered_df = pr_df[pr_df[\"Data_Type\"].isin([\"training\", \"testing\"])]\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Recall\",\n",
    "    y=\"Precision\",\n",
    "    hue=\"data_split\",\n",
    "    style=\"Model_Type\",\n",
    "    dashes={\"final\": (1, 0), \"shuffled\": (2, 2)},\n",
    "    data=filtered_df,\n",
    "    linewidth=4,  # Adjust the line width as needed\n",
    ")\n",
    "\n",
    "# Set legend location\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.ylim(bottom=0.0, top=1.02)\n",
    "plt.xlabel(\"Recall\", fontsize=24)\n",
    "plt.ylabel(\"Precision\", fontsize=24)\n",
    "plt.title(\"Precision vs. Recall Plate 4 Cell Type Classification\", fontsize=18)\n",
    "\n",
    "# Adjust x-axis ticks font size\n",
    "plt.xticks(fontsize=20)\n",
    "\n",
    "# Adjust y-axis ticks font size and labels\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{pr_curves_dir}/precision_recall_plate4_downsample.pdf\", dpi=500, bbox_inches='tight')\n",
    "\n",
    "# Avoid showing plot in notebook\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precision-recall curves for **training data only** per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curves with only testing and training data\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Combine model and data type as one column for plotting\n",
    "pr_df[\"data_split\"] = pr_df[\"Model_Type\"] + \" (\" + pr_df[\"Data_Type\"] + \")\"\n",
    "\n",
    "# Filter data frame to only show training versus testing\n",
    "filtered_df = pr_df[pr_df[\"Data_Type\"].isin([\"training\"])]\n",
    "\n",
    "# Define colors for each condition\n",
    "colors = {\"final (training)\": \"red\", \"shuffled (training)\": \"#ff7f0e\"}\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Recall\",\n",
    "    y=\"Precision\",\n",
    "    hue=\"data_split\",\n",
    "    style=\"Model_Type\",\n",
    "    dashes={\"final\": (1, 0), \"shuffled\": (2, 2)},\n",
    "    palette=colors,\n",
    "    data=filtered_df,\n",
    "    linewidth=2.5  # Adjust the line width as needed\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize=15)\n",
    "plt.ylim(bottom=0.0, top=1.02)\n",
    "plt.xlabel(\"Recall\", fontsize=18)\n",
    "plt.ylabel(\"Precision\", fontsize=18)\n",
    "plt.title(\"Precision vs. Recall Plate 4 Training Only\", fontsize=18)\n",
    "\n",
    "# Adjust x-axis ticks font size\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "# Adjust y-axis ticks font size and labels\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{pr_curves_dir}/precision_recall_plate4_downsample_only_training.png\", dpi=500)\n",
    "\n",
    "# Avoid showing plot in notebook\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Area Under the Precision Recall Curve (AUPRC) for the training and testing data (final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC for Training Data: 0.9269\n",
      "AUPRC for Testing Data: 0.8917\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe for the final model using the original pr_df\n",
    "final_model_df = pr_df[pr_df[\"Model_Type\"] == \"final\"]\n",
    "\n",
    "# Calculate AUPRC for training data\n",
    "training_data = final_model_df[final_model_df[\"Data_Type\"] == \"training\"]\n",
    "training_auprc = auc(training_data[\"Recall\"], training_data[\"Precision\"])\n",
    "\n",
    "# Calculate AUPRC for testing data\n",
    "testing_data = final_model_df[final_model_df[\"Data_Type\"] == \"testing\"]\n",
    "testing_auprc = auc(testing_data[\"Recall\"], testing_data[\"Precision\"])\n",
    "\n",
    "# Output the results\n",
    "print(f\"AUPRC for Training Data: {training_auprc:.4f}\")\n",
    "print(f\"AUPRC for Testing Data: {testing_auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precision-recall curves for holdout data per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom colors for the holdout plot to differentiate between the training/testing plot\n",
    "custom_colors = [\"#CC79A7\", \"#0072B2\", \"#009E73\", \"#882255\"]  # Pink, Dark Blue, Teal, Burgundy\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Combine model and data type as one column for plotting\n",
    "pr_df[\"data_split\"] = pr_df[\"Model_Type\"] + \" (\" + pr_df[\"Data_Type\"] + \")\"\n",
    "\n",
    "# Filter data frame to only show holdout datasets\n",
    "filtered_df = pr_df[pr_df[\"Data_Type\"].isin([\"holdout1\", \"holdout2\"])]\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Recall\",\n",
    "    y=\"Precision\",\n",
    "    hue=\"data_split\",\n",
    "    style=\"Model_Type\",\n",
    "    dashes={\"final\": (1, 0), \"shuffled\": (2, 2)},\n",
    "    data=filtered_df,\n",
    "    linewidth=2.5,  # Adjust the line width as needed\n",
    "    palette=custom_colors  # Apply the custom color palette\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize=13)\n",
    "plt.ylim(bottom=0.0, top=1.02)\n",
    "plt.xlabel(\"Recall\", fontsize=18)\n",
    "plt.ylabel(\"Precision\", fontsize=18)\n",
    "plt.title(\"Precision vs. Recall Plate 4 Holdout Datasets\", fontsize=18)\n",
    "\n",
    "# Adjust x-axis ticks font size\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "# Adjust y-axis ticks font size and labels\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{pr_curves_dir}/precision_recall_plate4_holdout_data_downsample.png\", dpi=500)\n",
    "\n",
    "# Close the plot to avoid showing it in the notebook\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate confusion matrix plots for each data set and model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating plot for Training Shuffled\n",
      "Generating plot for Testing Shuffled\n",
      "Generating plot for Holdout1 Shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/cellpainting_predicts_cardiac_fibrosis/5.machine_learning/1.evaluate_models/../../utils/training_utils.py:60: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_to_data, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating plot for Holdout2 Shuffled\n",
      "Generating plot for Training Final\n",
      "Generating plot for Testing Final\n",
      "Generating plot for Holdout1 Final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/cellpainting_predicts_cardiac_fibrosis/5.machine_learning/1.evaluate_models/../../utils/training_utils.py:60: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_to_data, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating plot for Holdout2 Final\n"
     ]
    }
   ],
   "source": [
    "# Define the subdirectory for this plot type within fig_dir\n",
    "con_matrix_dir = fig_dir / \"confusion_matrices\"\n",
    "con_matrix_dir.mkdir(parents=True, exist_ok=True)  # Create subdirectory if it doesn't exist\n",
    "\n",
    "# List of paths that contains each model\n",
    "path_list = list(models_dir.glob(\"*\"))\n",
    "\n",
    "# List of data sets to apply models to\n",
    "data_set_list = [\"training\", \"testing\", \"holdout1\", \"holdout2\"]\n",
    "\n",
    "# Metadata column used for classifying\n",
    "label = \"Metadata_cell_type\"\n",
    "\n",
    "# Iterate over each model path\n",
    "for model_path in path_list:\n",
    "    if model_path.is_dir() or model_path.suffix != \".joblib\":\n",
    "        continue  # Skip directories or files that are not model files\n",
    "    \n",
    "    # Iterate over each dataset\n",
    "    for data_set in data_set_list:\n",
    "        print(\n",
    "            \"Generating plot for\",\n",
    "            data_set.capitalize(),\n",
    "            model_path.stem.split(\"_\")[5].capitalize(),\n",
    "        )\n",
    "        # Generate confusion matrix data frame\n",
    "        df = generate_confusion_matrix_df(\n",
    "            model_path=model_path,\n",
    "            data_dir=data_dir,\n",
    "            encoder_path=encoder_path,\n",
    "            label=label,\n",
    "            data_set=data_set,\n",
    "        )\n",
    "\n",
    "        # Rename binary labels to failing versus healthy\n",
    "        df[\"True_Label\"] = df[\"True_Label\"].replace({0: \"Failing\", 1: \"Healthy\"})\n",
    "        df[\"Predicted_Label\"] = df[\"Predicted_Label\"].replace(\n",
    "            {0: \"Failing\", 1: \"Healthy\"}\n",
    "        )\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "\n",
    "        heatmap = sns.heatmap(\n",
    "            data=df.pivot(\n",
    "                index=\"True_Label\", columns=\"Predicted_Label\", values=\"Recall\"\n",
    "            ),\n",
    "            annot=df.pivot(\n",
    "                index=\"True_Label\", columns=\"Predicted_Label\", values=\"Count\"\n",
    "            ),\n",
    "            fmt=\".0f\",\n",
    "            cmap=\"Reds\",\n",
    "            square=True,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            xticklabels=[\"Failing\", \"Healthy\"],\n",
    "            yticklabels=[\"Failing\", \"Healthy\"],\n",
    "            linewidths=0.5,\n",
    "            annot_kws={\"size\": 16},  # Adjusted annotation font size\n",
    "            mask=(\n",
    "                df.pivot(\n",
    "                    index=\"True_Label\", columns=\"Predicted_Label\", values=\"Count\"\n",
    "                ).isna()\n",
    "            ),\n",
    "            cbar_kws={\"label\": \"Recall\"},\n",
    "        )\n",
    "\n",
    "        # Adjust color bar label font size\n",
    "        colorbar = heatmap.collections[0].colorbar\n",
    "        colorbar.set_label(\"Recall\", fontsize=14)  # Adjust colorbar label font size\n",
    "        colorbar.ax.tick_params(labelsize=12)  # Adjust colorbar tick labels font size\n",
    "\n",
    "        plt.title(\n",
    "            f\"Confusion Matrix for {data_set.capitalize()} Data - Model: {model_path.stem.split('_')[5].capitalize()}\",\n",
    "            fontsize=20,  # Adjusted title font size\n",
    "        )\n",
    "        plt.xticks(fontsize=14)  # Adjusted x-axis tick label font size\n",
    "        plt.yticks(fontsize=14)  # Adjusted y-axis tick label font size\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(\n",
    "            f\"{con_matrix_dir}/plate4_confusion_matrix_{data_set}_{model_path.stem.split('_')[5]}_downsample.png\",\n",
    "            dpi=500,\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat F1 scores from all datasets and model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shuffled\n",
      "Testing Shuffled\n",
      "Holdout1 Shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/cellpainting_predicts_cardiac_fibrosis/5.machine_learning/1.evaluate_models/../../utils/training_utils.py:60: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_to_data, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout2 Shuffled\n",
      "Training Final\n",
      "Testing Final\n",
      "Holdout1 Final\n",
      "Holdout2 Final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenna/cellpainting_predicts_cardiac_fibrosis/5.machine_learning/1.evaluate_models/../../utils/training_utils.py:60: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_to_data, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "# Define the subdirectory for this plot type within fig_dir\n",
    "f1_scores_dir = fig_dir / \"f1_scores\"\n",
    "f1_scores_dir.mkdir(parents=True, exist_ok=True)  # Create subdirectory if it doesn't exist\n",
    "\n",
    "# List of paths that contains each model\n",
    "path_list = list(models_dir.glob(\"*\"))\n",
    "\n",
    "# Metadata column used for classifying\n",
    "label = \"Metadata_cell_type\"\n",
    "\n",
    "# list for the f1 score dataframes per model and data split\n",
    "f1_scores_dfs = []\n",
    "\n",
    "# Iterate over each model path\n",
    "for model_path in path_list:\n",
    "    if model_path.is_dir() or model_path.suffix != \".joblib\":\n",
    "        continue  # Skip directories or files that are not model files\n",
    "    \n",
    "    # Iterate over each dataset\n",
    "    for data_set in data_set_list:\n",
    "        print(data_set.capitalize(), model_path.stem.split(\"_\")[5].capitalize())\n",
    "        \n",
    "        # Generate f1 score data frame\n",
    "        f1_scores_df = generate_f1_score_df(\n",
    "            model_path=model_path,\n",
    "            data_dir=data_dir,\n",
    "            encoder_path=encoder_path,\n",
    "            label=label,\n",
    "            data_set=data_set,\n",
    "        )\n",
    "\n",
    "        # Rename binary labels to failing versus healthy\n",
    "        f1_scores_df = f1_scores_df.rename(columns={0: \"Failing\", 1: \"Healthy\"})\n",
    "\n",
    "        # Add columns for model and data_set\n",
    "        f1_scores_df[\"Model\"] = model_path.stem.split(\"_\")[5].capitalize()\n",
    "        f1_scores_df[\"Data_Set\"] = data_set.capitalize()\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        f1_scores_dfs.append(f1_scores_df)\n",
    "\n",
    "concat_f1_scores = pd.concat(f1_scores_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if output looks correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Failing</th>\n",
       "      <th>Healthy</th>\n",
       "      <th>Weighted</th>\n",
       "      <th>Model</th>\n",
       "      <th>Data_Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.606624</td>\n",
       "      <td>0.448968</td>\n",
       "      <td>0.547585</td>\n",
       "      <td>Shuffled</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.591913</td>\n",
       "      <td>0.453401</td>\n",
       "      <td>0.540050</td>\n",
       "      <td>Shuffled</td>\n",
       "      <td>Testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.564753</td>\n",
       "      <td>0.408003</td>\n",
       "      <td>0.515821</td>\n",
       "      <td>Shuffled</td>\n",
       "      <td>Holdout1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.560782</td>\n",
       "      <td>0.447348</td>\n",
       "      <td>0.518543</td>\n",
       "      <td>Shuffled</td>\n",
       "      <td>Holdout2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.921621</td>\n",
       "      <td>0.877732</td>\n",
       "      <td>0.905185</td>\n",
       "      <td>Final</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Failing   Healthy  Weighted     Model  Data_Set\n",
       "0  0.606624  0.448968  0.547585  Shuffled  Training\n",
       "1  0.591913  0.453401  0.540050  Shuffled   Testing\n",
       "2  0.564753  0.408003  0.515821  Shuffled  Holdout1\n",
       "3  0.560782  0.447348  0.518543  Shuffled  Holdout2\n",
       "4  0.921621  0.877732  0.905185     Final  Training"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(concat_f1_scores.shape)\n",
    "concat_f1_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot F1 scores for only testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set palette for the f1 scores plot\n",
    "palette = sns.color_palette(\"Paired\")[:2]\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set y-axis limits\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Filter the data to include only training and testing sets\n",
    "filtered_data = concat_f1_scores[\n",
    "    concat_f1_scores[\"Data_Set\"].isin([\"Training\", \"Testing\"])\n",
    "]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=\"Model\", y=\"Weighted\", hue=\"Data_Set\", data=filtered_data, palette=palette\n",
    ")\n",
    "\n",
    "# Add the weighted values above the bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(\n",
    "            f\"{height:.2f}\",\n",
    "            (p.get_x() + p.get_width() / 2.0, height),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            xytext=(0, 10),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "plt.title(\"Weighted F1 Score by Model Type\")\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.ylabel(\"Weighted F1 Score\")\n",
    "plt.legend(title=\"Data Set\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{f1_scores_dir}/f1_scores_plate4_downsample.png\", dpi=500)\n",
    "\n",
    "# Avoid showing plot in the notebook\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot F1 scores for holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set palette for the f1 scores plot\n",
    "palette = sns.color_palette(\"Paired\")[2:4]\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set y-axis limits\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Filter the data to include only hold-out sets\n",
    "filtered_data = concat_f1_scores[\n",
    "    concat_f1_scores[\"Data_Set\"].isin([\"Holdout1\", \"Holdout2\"])\n",
    "]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=\"Model\", y=\"Weighted\", hue=\"Data_Set\", data=filtered_data, palette=palette\n",
    ")\n",
    "\n",
    "# Add the weighted values above the bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(\n",
    "            f\"{height:.2f}\",\n",
    "            (p.get_x() + p.get_width() / 2.0, height),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            xytext=(0, 10),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "plt.title(\"Weighted F1 Score by Model Type\")\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.ylabel(\"Weighted F1 Score\")\n",
    "plt.legend(title=\"Data Set\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{f1_scores_dir}/f1_scores_plate4_holdout_data_downsample.png\", dpi=500)\n",
    "\n",
    "# Avoid showing plot in the notebook\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate accuracy scores specifically from the whole plate 4 dataset per heart and save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shuffled\n",
      "Testing Shuffled\n",
      "Holdout1 Shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2128881/1026893374.py:20: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout2 Shuffled\n",
      "Training Final\n",
      "Testing Final\n",
      "Holdout1 Final\n",
      "Holdout2 Final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2128881/1026893374.py:20: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "# List of paths that contains each model\n",
    "path_list = list(models_dir.glob(\"*\"))\n",
    "\n",
    "# Metadata column used for classifying\n",
    "label = \"Metadata_cell_type\"\n",
    "\n",
    "# List for all accuracy dataframes for concat\n",
    "accuracy_dfs = []\n",
    "\n",
    "# Iterate over each model path\n",
    "for model_path in path_list:\n",
    "    if model_path.is_dir() or model_path.suffix != \".joblib\":\n",
    "        continue  # Skip directories or files that are not model files\n",
    "    # Iterate over each dataset\n",
    "    for data_set in data_set_list:\n",
    "        print(data_set.capitalize(), model_path.stem.split(\"_\")[5].capitalize())\n",
    "\n",
    "        # Load the dataset\n",
    "        data_path = data_dir / f\"{data_set}_data.csv\"\n",
    "        data_df = pd.read_csv(data_path)\n",
    "\n",
    "        # Group the data by heart number\n",
    "        grouped_data = data_df.groupby(\"Metadata_heart_number\")\n",
    "\n",
    "        # Iterate over each group (heart number)\n",
    "        for heart_number, df_heart in grouped_data:\n",
    "            # Generate accuracy data frame\n",
    "            accuracy_df = generate_accuracy_score_df(\n",
    "                model_path=model_path,\n",
    "                data_set=df_heart,\n",
    "                encoder_path=encoder_path,\n",
    "                label=label,\n",
    "            )\n",
    "\n",
    "            # Rename binary labels to failing versus healthy\n",
    "            accuracy_df = accuracy_df.rename(columns={0: \"Failing\", 1: \"Healthy\"})\n",
    "\n",
    "            # Add columns for model, data_set, and heart number\n",
    "            accuracy_df[\"Model\"] = model_path.stem.split(\"_\")[5].capitalize()\n",
    "            accuracy_df[\"Data_Set\"] = data_set.capitalize()\n",
    "            accuracy_df[\"Heart_Number\"] = heart_number\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            accuracy_dfs.append(accuracy_df)\n",
    "\n",
    "# Merge all of the accuracy scores across model type and data splot\n",
    "concat_accuracy_scores = pd.concat(accuracy_dfs, ignore_index=True)\n",
    "\n",
    "# Save results for visualization\n",
    "concat_accuracy_scores.to_csv(f\"{results_dir}/accuracy_scores_per_heart.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
